{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1RKefqKpUtY"
      },
      "source": [
        "# import library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ-hdEr2l5yD"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpIchvdTmJue",
        "outputId": "a1132981-6dbb-4f48-e5a0-21eb528ccf52"
      },
      "source": [
        "# loading training set features\n",
        "f = open(\"Datasets/train_set_features.pkl\", \"rb\")\n",
        "train_set_features2 = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "# reducing feature vector length \n",
        "features_STDs = np.std(a=train_set_features2, axis=0)\n",
        "train_set_features = train_set_features2[:, features_STDs > 52.3]\n",
        "\n",
        "# changing the range of data between 0 and 1\n",
        "train_set_features = np.divide(train_set_features, train_set_features.max())\n",
        "\n",
        "# loading training set labels\n",
        "f = open(\"Datasets/train_set_labels.pkl\", \"rb\")\n",
        "train_set_labels = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "# ------------\n",
        "# loading test set features\n",
        "f = open(\"Datasets/test_set_features.pkl\", \"rb\")\n",
        "test_set_features2 = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "# reducing feature vector length \n",
        "features_STDs = np.std(a=test_set_features2, axis=0)\n",
        "test_set_features = test_set_features2[:, features_STDs > 48]\n",
        "\n",
        "# changing the range of data between 0 and 1\n",
        "test_set_features = np.divide(test_set_features, test_set_features.max())\n",
        "\n",
        "# loading test set labels\n",
        "f = open(\"Datasets/test_set_labels.pkl\", \"rb\")\n",
        "test_set_labels = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "# ------------\n",
        "# preparing our training and test sets - joining datasets and lables\n",
        "train_set = []\n",
        "test_set = []\n",
        "\n",
        "for i in range(len(train_set_features)):\n",
        "    label = np.array([0,0,0,0])\n",
        "    label[int(train_set_labels[i])] = 1\n",
        "    label = label.reshape(4,1)\n",
        "    train_set.append((train_set_features[i].reshape(102,1), label))\n",
        "    \n",
        "\n",
        "for i in range(len(test_set_features)):\n",
        "    label = np.array([0,0,0,0])\n",
        "    label[int(test_set_labels[i])] = 1\n",
        "    label = label.reshape(4,1)\n",
        "    test_set.append((test_set_features[i].reshape(102,1), label))\n",
        "\n",
        "# shuffle\n",
        "random.shuffle(train_set)\n",
        "random.shuffle(test_set)\n",
        "\n",
        "# print size\n",
        "print(len(train_set)) #1962\n",
        "print(len(test_set)) #662\n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1962\n",
            "662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2bgAP1mpk0U"
      },
      "source": [
        "### Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfUzwPEjAcLP"
      },
      "source": [
        "minimize_train_set=train_set[:200]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKUu4eermp2s",
        "outputId": "a15f1d6b-5745-47e5-cd90-6478e0e0ae68"
      },
      "source": [
        "def sigmoid(x):\n",
        "    ans=1/(1+np.exp(-x))\n",
        "    return ans\n",
        "def result(x,w,b):\n",
        "    return np.dot(w,x)+b\n",
        "\n",
        "np.random.seed(1)\n",
        "n_x=102\n",
        "n_h_1=150\n",
        "n_h_2=60\n",
        "n_y=4\n",
        "#intialize the layers here\n",
        "W1 = np.random.randn(n_h_1,n_x) * 0.01\n",
        "b1 = np.zeros((n_h_1,1))\n",
        "W2 = np.random.randn(n_h_2,n_h_1) * 0.01\n",
        "b2 = np.zeros((n_h_2,1))\n",
        "W3 = np.random.randn(n_y,n_h_2) * 0.01\n",
        "b3 = np.zeros((n_y,1))\n",
        "out=[]\n",
        "for i in range(len(minimize_train_set)):\n",
        "      reshape_train=minimize_train_set[i][0]\n",
        "      Z1=result(reshape_train,W1,b1)\n",
        "      S1=sigmoid(Z1)\n",
        "      Z2=result(S1,W2,b2)\n",
        "      S2=sigmoid(Z2)\n",
        "      Z3=result(S2,W3,b3)\n",
        "      S3=sigmoid(Z3)\n",
        "      out.append(S3)\n",
        "\n",
        "index=[]  \n",
        "for i in out:\n",
        "  k=i.tolist()\n",
        "  max1=0\n",
        "  for count in k:\n",
        "    if(count[0]>max1):\n",
        "      max1=count[0]\n",
        "  for j in range(len(k)):\n",
        "     if(k[j][0]==max1):\n",
        "       index.append(j)\n",
        "counter=0       \n",
        "for count in range(len(index)):\n",
        "  if(minimize_train_set[count][1][index[count]]==1):\n",
        "    counter+=1       \n",
        "print(\"result \"+str(counter/200))\n",
        "       \n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result 0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4qgAgmqrs0d"
      },
      "source": [
        "###part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOKWm_4wcZdF"
      },
      "source": [
        "epoch=3\n",
        "batch_size=10\n",
        "batch_num=int(200/10)\n",
        "learning_rate=10000\n",
        "for epoch_count in range(epoch):\n",
        "  #shuffle\n",
        "  out=[]\n",
        "  random.shuffle(train_set)\n",
        "  for batch_count in range(batch_size):\n",
        "    grad_W1 = np.zeros((n_h_1,n_x))\n",
        "    grad_W2 = np.zeros((n_h_2,n_h_1))\n",
        "    grad_W3 = np.zeros((n_y,n_h_2))\n",
        "    grad_b1 = np.zeros((n_h_1,1))\n",
        "    grad_b2 = np.zeros((n_h_2,1))\n",
        "    grad_b3 = np.zeros((n_y,1)) \n",
        "    for i in range(batch_num): \n",
        "      reshape_train=minimize_train_set[batch_count*10+i][0]\n",
        "      reshape_train_lables=minimize_train_set[batch_count*10+i][1]\n",
        "      Z1=result(reshape_train,W1,b1)\n",
        "      S1=sigmoid(Z1)\n",
        "      Z2=result(S1,W2,b2)\n",
        "      S2=sigmoid(Z2)\n",
        "      Z3=result(S2,W3,b3)\n",
        "      S3=sigmoid(Z3)\n",
        "      out.append(S3)\n",
        "      for j in range(grad_W3.shape[0]):\n",
        "          for k in range(grad_W3.shape[1]):\n",
        "              grad_W3[j, k] += 2 * (S3[j, 0] - label[j, 0]) * S3[j, 0] * (1 - S3[j, 0]) * S2[k, 0]\n",
        "      \n",
        "      # bias\n",
        "      for j in range(grad_b3.shape[0]):\n",
        "              grad_b3[j, 0] += 2 * (S3[j, 0] - label[j, 0]) * S3[j, 0] * (1 - S3[j, 0])\n",
        "      \n",
        "      # ---- 3rd layer\n",
        "      # activation\n",
        "      delta_3 = np.zeros((n_h_2, 1))\n",
        "      for k in range(n_h_2):\n",
        "          for j in range(n_y):\n",
        "              delta_3[k, 0] += 2 * (S3[j, 0] - label[j, 0]) * S3[j, 0] * (1 - S3[j, 0]) * W3[j, k]\n",
        "      \n",
        "      # weight\n",
        "      for k in range(grad_W2.shape[0]):\n",
        "          for m in range(grad_W2.shape[1]):\n",
        "              grad_W2[k, m] += delta_3[k, 0] * S2[k,0] * (1 - S2[k, 0]) * S1[m, 0]\n",
        "      \n",
        "      # bias\n",
        "      for k in range(grad_b2.shape[0]):\n",
        "              grad_b2[k, 0] += delta_3[k, 0] * S2[k, 0] * (1 - S2[k, 0])\n",
        "              \n",
        "      # ---- 2nd layer\n",
        "      # activation\n",
        "      delta_2 = np.zeros((n_h_1, 1))\n",
        "      for m in range(n_h_1):\n",
        "          for k in range(n_h_2):\n",
        "              delta_2[m, 0] += delta_3[k, 0] * S2[k][0] * (1 - S2[k, 0]) * W2[k, m]\n",
        "      # weight\n",
        "      for m in range(grad_W1.shape[0]):\n",
        "          for v in range(grad_W1.shape[1]):\n",
        "              grad_W1[m, v] += delta_2[m, 0] * S1[m,0] * (1 - S1[m, 0]) * reshape_train[v,0]\n",
        "              \n",
        "      # bias\n",
        "      for m in range(grad_b1.shape[0]):\n",
        "              grad_b1[m, 0] += delta_2[m, 0] * S1[m, 0] * (1 - S1[m, 0])\n",
        "              \n",
        "      W3 = W3 - (learning_rate * (grad_W3 / batch_size))\n",
        "      W2 = W2 - (learning_rate * (grad_W2 / batch_size))\n",
        "      W1 = W1 - (learning_rate * (grad_W1 / batch_size))\n",
        "      b3 = b3 - (learning_rate * (grad_b3 / batch_size))\n",
        "      b2 = b2 - (learning_rate * (grad_b2 / batch_size))\n",
        "      b1 = b1 - (learning_rate * (grad_b1 / batch_size))\n",
        "index=[]\n",
        "  \n",
        "for i in out:\n",
        "  k=i.tolist()\n",
        "  max1=0\n",
        "  for count in k:\n",
        "    if(count[0]>max1):\n",
        "      max1=count[0]\n",
        "  for j in range(len(k)):\n",
        "     if(k[j][0]==max1):\n",
        "       index.append(j)\n",
        "counter=0       \n",
        "for count in range(len(index)):\n",
        "  if(minimize_train_set[count][1][index[count]]==1):\n",
        "    counter+=1       \n",
        "print(\"result \"+str(counter/200))    \n",
        "  \n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZfr1T5QpSzf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiF5vlDrtbdy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}